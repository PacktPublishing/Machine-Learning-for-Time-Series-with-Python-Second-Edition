{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b056a15-b994-4685-97eb-278dcece0985",
   "metadata": {},
   "source": [
    "We will implement the code examples for:\n",
    "* sktime: For research-grade rigor and composable pipelines.\n",
    "* darts: For rapid prototyping and ease of use.\n",
    "* mlforecast: For high-performance scaling using Numba.\n",
    "* autogluon: For automated benchmarking and ensembling.\n",
    "* skforecast: For direct integration with scikit-learn regressors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928a053c-13d7-4a9f-966b-f9034b582a13",
   "metadata": {},
   "source": [
    "# Sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80611ea8-eb50-4dcd-b272-46cd36131ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/anaconda3/envs/ml_timeseries/lib/python3.12/site-packages/sktime/forecasting/model_selection/_base.py:76: UserWarning: Parameter n_jobs of ForecastingGridSearchCV has been removed in sktime 0.27.0 and is no longer used. It is ignored when passed. Instead, the backend and backend_params parameters should be used to pass n_jobs or other parallelization parameters.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'error': 'add', 'trend': 'mul'}\n",
      "Prediction (first 5 values):\n",
      "         #Passengers\n",
      "1960-01   466.578473\n",
      "1960-02   470.358248\n",
      "1960-03   474.168644\n",
      "1960-04   478.009907\n",
      "1960-05   481.882289\n",
      "\n",
      "MAPE on validation set: 13.23%\n"
     ]
    }
   ],
   "source": [
    "from sktime.forecasting.model_selection import ForecastingGridSearchCV, SlidingWindowSplitter\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.forecasting.ets import AutoETS\n",
    "from sktime.datasets import load_airline\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load data in the pd.Series format sktime expects\n",
    "y = load_airline()\n",
    "# 2. Split into training and validation (e.g., last 12 months as validation)\n",
    "y_train, y_val = temporal_train_test_split(y, test_size=12)\n",
    "\n",
    "# 2. Define the forecaster (our model)\n",
    "# The API feels very similar to scikit-learn\n",
    "forecaster = AutoETS()\n",
    "\n",
    "# 3. Define the temporal cross-validation strategy from Chapter 3\n",
    "cv = SlidingWindowSplitter(window_length=50, step_length=12, fh=1)\n",
    "\n",
    "# 4. Define the parameter grid to search\n",
    "param_grid = {\"error\": [\"add\", \"mul\"], \"trend\": [\"add\", \"mul\"]}\n",
    "\n",
    "# 5. Set up and run the grid search\n",
    "# This combines the model, CV, and params into a single, robust workflow\n",
    "gscv = ForecastingGridSearchCV(\n",
    "    forecaster=forecaster,\n",
    "    cv=cv,\n",
    "    param_grid=param_grid,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gscv.fit(y_train)\n",
    "print(f\"Best parameters found: {gscv.best_params_}\")\n",
    "\n",
    "# 6. Forecast the validation horizon\n",
    "fh = list(range(1, len(y_val)+1))  # forecast horizon = length of validation\n",
    "y_pred = gscv.predict(fh=fh)\n",
    "\n",
    "# 7. Compute performance (MAPE)\n",
    "mape = mean_absolute_percentage_error(y_val, y_pred) * 100\n",
    "\n",
    "# 8. Show first 5 predictions like your previous output\n",
    "prediction_preview = pd.DataFrame(y_pred).head()\n",
    "prediction_preview.columns = [\"#Passengers\"]\n",
    "print(\"Prediction (first 5 values):\")\n",
    "print(prediction_preview)\n",
    "print(f\"\\nMAPE on validation set: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a45e8-88a8-4969-b71b-be146a1e3c68",
   "metadata": {},
   "source": [
    "# Darts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4842c0aa-8da9-40f3-90a4-836df0a867e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/anaconda3/envs/ml_timeseries/lib/python3.12/site-packages/threadpoolctl.py:1226: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (first 5 values):\n",
      "            #Passengers\n",
      "Month                  \n",
      "1958-01-01   357.332132\n",
      "1958-02-01   345.819810\n",
      "1958-03-01   398.634260\n",
      "1958-04-01   390.191635\n",
      "1958-05-01   396.345978\n",
      "\n",
      "shape: (5, 1, 1), freq: MS, size: 40.00 B\n",
      "MAPE on validation set: 5.11%\n"
     ]
    }
   ],
   "source": [
    "from darts.datasets import AirPassengersDataset\n",
    "from darts.models import ExponentialSmoothing\n",
    "from darts import TimeSeries\n",
    "from darts.metrics import mape\n",
    "\n",
    "# 1. Load data into a TimeSeries object, darts' standard container\n",
    "series: TimeSeries = AirPassengersDataset().load()\n",
    "\n",
    "# 2. Split the data, holding out the last 36 months for validation\n",
    "train, val = series[:-36], series[-36:]\n",
    "\n",
    "# 3. Create and train the model using the unified API\n",
    "# The code remains the same even if you swap ExponentialSmoothing with a deep learning model\n",
    "model = ExponentialSmoothing()\n",
    "model.fit(train)\n",
    "\n",
    "# 4. Predict the next len(val) timesteps\n",
    "prediction = model.predict(len(val))\n",
    "\n",
    "# 5. Inspect the forecast and evaluate its performance\n",
    "print(\"Prediction (first 5 values):\")\n",
    "print(prediction[:5])\n",
    "print(f\"MAPE on validation set: {mape(val, prediction):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f5949d-3db8-4edf-a42b-da14e0f0ef92",
   "metadata": {},
   "source": [
    "# mlforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33ca4ec1-69f0-4046-9ea9-ca06234f4c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (first 5 values):\n",
      "  unique_id         ds      lgbm\n",
      "0      id_0 2000-05-10  4.256834\n",
      "1      id_0 2000-05-11  5.278685\n",
      "2      id_0 2000-05-12  6.199548\n",
      "3      id_0 2000-05-13  0.173573\n",
      "4      id_0 2000-05-14  1.253689\n",
      "\n",
      "MAPE per series:\n",
      "unique_id\n",
      "id_0     51.423117\n",
      "id_1     21.437474\n",
      "id_2      8.048068\n",
      "id_3      9.251338\n",
      "id_4     24.030832\n",
      "id_5     11.027903\n",
      "id_6      5.535145\n",
      "id_7      9.775673\n",
      "id_8      7.743888\n",
      "id_9    109.184316\n",
      "Name: ape, dtype: float64\n",
      "\n",
      "Overall MAPE: 25.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zs/dpjs6jdx52xgnf97mcsn19240000gp/T/ipykernel_26706/2158128676.py:14: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_df = series_df.groupby(\"unique_id\", group_keys=False, observed=True).apply(lambda x: x.iloc[:-h])\n",
      "/var/folders/zs/dpjs6jdx52xgnf97mcsn19240000gp/T/ipykernel_26706/2158128676.py:15: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  val_df = series_df.groupby(\"unique_id\", group_keys=False, observed=True).apply(lambda x: x.iloc[-h:])\n",
      "/var/folders/zs/dpjs6jdx52xgnf97mcsn19240000gp/T/ipykernel_26706/2158128676.py:51: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mape_per_series = merged.groupby('unique_id')['ape'].mean() * 100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.lag_transforms import RollingMean\n",
    "from mlforecast.utils import generate_daily_series\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# 1. Generate synthetic daily series\n",
    "series_df = generate_daily_series(n_series=10, min_length=100, max_length=150)\n",
    "\n",
    "# 2. Split into train/validation (last 14 days for validation)\n",
    "h = 14\n",
    "train_df = series_df.groupby(\"unique_id\", group_keys=False, observed=True).apply(lambda x: x.iloc[:-h])\n",
    "val_df = series_df.groupby(\"unique_id\", group_keys=False, observed=True).apply(lambda x: x.iloc[-h:])\n",
    "\n",
    "\n",
    "# 3. Define LightGBM model\n",
    "model = lgb.LGBMRegressor(\n",
    "    random_state=42,\n",
    "    n_jobs=1,\n",
    "    num_leaves=64,\n",
    "    min_child_samples=5,\n",
    "    learning_rate=0.05,\n",
    "    verbose=-1,\n",
    "    force_row_wise=True\n",
    ")\n",
    "\n",
    "# 4. Define MLForecast\n",
    "fcst = MLForecast(\n",
    "    models={'lgbm': model},\n",
    "    freq='D',\n",
    "    lags=[1,7,14],\n",
    "    lag_transforms={1: [RollingMean(window_size=7, min_samples=1)]},\n",
    "    date_features=['dayofweek','month'],\n",
    "    num_threads=1\n",
    ")\n",
    "\n",
    "# 5. Fit on training data\n",
    "fcst.fit(train_df)\n",
    "\n",
    "# 6. Predict validation horizon\n",
    "y_pred = fcst.predict(h=h)\n",
    "\n",
    "# 7. Merge predictions with validation\n",
    "merged = val_df.merge(y_pred, on=['unique_id','ds'])\n",
    "merged = merged.rename(columns={'y': 'y_true'})  # rename target column for clarity\n",
    "\n",
    "# 8. Compute per-series and overall MAPE\n",
    "merged['ape'] = np.abs(merged['y_true'] - merged['lgbm']) / merged['y_true']\n",
    "mape_per_series = merged.groupby('unique_id')['ape'].mean() * 100\n",
    "overall_mape = merged['ape'].mean() * 100\n",
    "\n",
    "# 9. Show first 5 predictions\n",
    "prediction_preview = y_pred.head()\n",
    "print(\"Prediction (first 5 values):\")\n",
    "print(prediction_preview)\n",
    "\n",
    "print(f\"\\nMAPE per series:\\n{mape_per_series}\")\n",
    "print(f\"\\nOverall MAPE: {overall_mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf30a45-20f8-459e-910b-10eca7686aa7",
   "metadata": {},
   "source": [
    "# Autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d7b7139-2016-4194-a486-76d74164e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training...\n",
      "AutoGluon will save models to '/Users/ben/Machine-Learning-for-Time-Series-with-Python-Second-Edition/chapter4/AutogluonModels/ag-20251206_130136'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.11\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 22.6.0: Fri Nov 15 17:21:49 PST 2024; root:xnu-8796.141.3.709.7~2/RELEASE_X86_64\n",
      "CPU Count:          4\n",
      "GPU Count:          0\n",
      "Memory Avail:       4.69 GB / 16.00 GB (29.3%)\n",
      "Disk Space Avail:   202.49 GB / 465.63 GB (43.5%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': MASE,\n",
      " 'hyperparameters': {'Naive': {}, 'SeasonalNaive': {}},\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 1,\n",
      " 'prediction_length': 24,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'target',\n",
      " 'verbosity': 2}\n",
      "\n",
      "Inferred time series frequency: 'D'\n",
      "Provided train_data has 300 rows, 3 time series. Median time series length is 100 (min=100, max=100). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'target'\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'MASE'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-12-06 13:01:39\n",
      "Models that will be trained: ['Naive', 'SeasonalNaive']\n",
      "Training timeseries model Naive. \n",
      "\t-1.0280       = Validation score (-MASE)\n",
      "\t0.03    s     = Training runtime\n",
      "\t6.39    s     = Validation (prediction) runtime\n",
      "Training timeseries model SeasonalNaive. \n",
      "\t-1.4705       = Validation score (-MASE)\n",
      "\t0.02    s     = Training runtime\n",
      "\t0.03    s     = Validation (prediction) runtime\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'Naive': 1.0}\n",
      "\t-1.0280       = Validation score (-MASE)\n",
      "\t0.31    s     = Training runtime\n",
      "\t6.39    s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['Naive', 'SeasonalNaive', 'WeightedEnsemble']\n",
      "Total runtime: 6.88 s\n",
      "Best model: Naive\n",
      "Best model score: -1.0280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance leaderboard:\n",
      "              model  score_val  pred_time_val  fit_time_marginal  fit_order\n",
      "0  WeightedEnsemble  -1.028021       6.392401           0.313400          3\n",
      "1             Naive  -1.028021       6.392401           0.029221          1\n",
      "2     SeasonalNaive  -1.470471       0.032117           0.020691          2\n"
     ]
    }
   ],
   "source": [
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Sample data\n",
    "ids = [\"A\", \"B\", \"C\"]\n",
    "timestamps = pd.to_datetime(pd.date_range(\"2023-01-01\", periods=100))\n",
    "data = []\n",
    "for item_id in ids:\n",
    "    target = np.random.randn(100).cumsum()\n",
    "    data.append(pd.DataFrame({\"item_id\": item_id, \"timestamp\": timestamps, \"target\": target}))\n",
    "\n",
    "df = pd.concat(data)\n",
    "\n",
    "# 2. Convert to AutoGluon format\n",
    "train_data = TimeSeriesDataFrame.from_data_frame(\n",
    "    df,\n",
    "    id_column=\"item_id\",\n",
    "    timestamp_column=\"timestamp\"\n",
    ")\n",
    "\n",
    "# 3. Fit a predictor with only truly lightweight models\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=24,\n",
    "    target=\"target\",\n",
    "    eval_metric=\"MASE\",\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "# Only train models guaranteed to be very light\n",
    "predictor.fit(\n",
    "    train_data,\n",
    "    hyperparameters={\n",
    "        \"Naive\": {},\n",
    "        \"SeasonalNaive\": {}\n",
    "    },\n",
    "    num_val_windows=1,       # keep 1 validation window\n",
    "    refit_every_n_windows=None  # avoid repeated refitting, saves memory\n",
    ")\n",
    "\n",
    "# 4. Leaderboard\n",
    "print(\"Model performance leaderboard:\")\n",
    "print(predictor.leaderboard())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79700f33-5a6f-4742-968b-58e40564b1d1",
   "metadata": {},
   "source": [
    "# Skforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24922279-1dc1-429c-91f0-0804072b66d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecasts (first 5 values):\n",
      "1958-01-01    352.06\n",
      "1958-02-01    350.19\n",
      "1958-03-01    422.17\n",
      "1958-04-01    431.52\n",
      "1958-05-01    429.42\n",
      "Freq: MS, Name: pred, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from skforecast.recursive import ForecasterRecursive\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sktime.datasets import load_airline\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load data as a standard pandas Series\n",
    "y = load_airline()\n",
    "y.index = pd.to_datetime(y.index.to_timestamp()) # Ensure datetime index\n",
    "\n",
    "# 2. Create the scikit-learn model you want to use\n",
    "regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# 3. Create the forecaster, passing the regressor and specifying lags\n",
    "forecaster = ForecasterRecursive(\n",
    "    estimator=regressor,\n",
    "    lags=15  # Create 15 lag features\n",
    ")\n",
    "\n",
    "# 4. Fit the forecaster on the training data\n",
    "forecaster.fit(y=y[:-36])\n",
    "\n",
    "# 5. Predict the next 36 steps\n",
    "predictions = forecaster.predict(steps=36)\n",
    "\n",
    "print(\"Forecasts (first 5 values):\")\n",
    "print(predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1fc683-23b7-43f1-8911-b4038c39fdde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
