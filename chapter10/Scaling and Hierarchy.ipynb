{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1246cdd8-5b9a-4b01-820e-5d0c763de54b",
   "metadata": {},
   "source": [
    "# Distributed Forecasting (Ray + StatsForecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e773860-6d3c-4d30-8803-9978e77f0d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 14:34:45,455\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset with 2000 rows across 20 unique series.\n",
      "Distributing 20 series across Ray cluster...\n",
      "Done! Processed 20 series in 29.64 seconds.\n",
      "  unique_id         ds  AutoARIMA  Naive\n",
      "0   store_0 2023-04-11       24.5   26.0\n",
      "1   store_0 2023-04-12       24.5   26.0\n",
      "2   store_0 2023-04-13       24.5   26.0\n",
      "3   store_0 2023-04-14       24.5   26.0\n",
      "4   store_0 2023-04-15       24.5   26.0\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, Naive\n",
    "import time\n",
    "\n",
    "# 1. Initialize Ray\n",
    "# We use ignore_reinit_error=True in case you run the cell twice\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# 2. Generate Synthetic Data (20 unique series)\n",
    "n_series = 20\n",
    "horizon = 7\n",
    "dates = pd.date_range(start='2023-01-01', periods=100, freq='D')\n",
    "\n",
    "series_list = []\n",
    "for i in range(n_series):\n",
    "    # Create random demand patterns\n",
    "    y = np.random.poisson(lam=20 + np.random.rand()*10, size=len(dates))\n",
    "    series_list.append(pd.DataFrame({\n",
    "        'unique_id': f'store_{i}',\n",
    "        'ds': dates, \n",
    "        'y': y\n",
    "    }))\n",
    "\n",
    "df = pd.concat(series_list)\n",
    "print(f\"Generated dataset with {len(df)} rows across {n_series} unique series.\")\n",
    "\n",
    "# 3. Define Distributed Logic\n",
    "@ray.remote\n",
    "def train_and_predict_chunk(df_chunk, h):\n",
    "    models = [AutoARIMA(season_length=7), Naive()]\n",
    "    \n",
    "    # Correct Initialization\n",
    "    sf = StatsForecast(models=models, freq='D', n_jobs=1) \n",
    "    \n",
    "    # Correct Execution\n",
    "    return sf.forecast(df=df_chunk, h=h)\n",
    "\n",
    "# 4. Scatter (Map)\n",
    "print(f\"Distributing 20 series across Ray cluster...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Split unique_ids into chunks (e.g., 4 chunks)\n",
    "unique_ids = df['unique_id'].unique()\n",
    "chunks = np.array_split(unique_ids, 4) \n",
    "\n",
    "futures = []\n",
    "for chunk_ids in chunks:\n",
    "    # Filter data for this chunk\n",
    "    df_subset = df[df['unique_id'].isin(chunk_ids)]\n",
    "    # Dispatch remote task\n",
    "    futures.append(train_and_predict_chunk.remote(df_subset, h=horizon))\n",
    "\n",
    "# 5. Gather (Reduce)\n",
    "results = ray.get(futures)\n",
    "forecast_df = pd.concat(results)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"Done! Processed {n_series} series in {total_time:.2f} seconds.\")\n",
    "print(forecast_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0bdd3-a51a-4008-8b02-a4b8f466830e",
   "metadata": {},
   "source": [
    "# Hierarchical Reconciliation (MinT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47cc4ab1-35d5-4cc1-b291-b5e5f4174fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hierarchical Reconciliation ---\n",
      "\n",
      "Checking Coherency for date: 2023-01-01\n",
      "Using column: y_hat/MinTrace_method-ols\n",
      "                y_hat  y_hat/MinTrace_method-ols\n",
      "unique_id                                       \n",
      "Total      306.408645                 304.537567\n",
      "RegionA    100.586664                 102.457742\n",
      "RegionB    200.208748                 202.079826\n",
      "\n",
      "MinTrace Total: 304.5376\n",
      "Sum of MinTrace Regions: 304.5376\n",
      "Difference: 0.0000000000 (Should be near zero)\n"
     ]
    }
   ],
   "source": [
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.methods import BottomUp, MinTrace\n",
    "\n",
    "# 1. Construct a simple Hierarchy\n",
    "print(\"\\n--- Hierarchical Reconciliation ---\")\n",
    "\n",
    "n_obs = 50\n",
    "dates_h = pd.date_range('2023-01-01', periods=n_obs, freq='D')\n",
    "\n",
    "# Region A ~ Normal(100, 10), Region B ~ Normal(200, 10)\n",
    "y_a = np.random.normal(100, 10, n_obs)\n",
    "y_b = np.random.normal(200, 10, n_obs)\n",
    "y_total = y_a + y_b \n",
    "\n",
    "# Create DataFrame\n",
    "df_h = pd.DataFrame([\n",
    "    {'unique_id': 'Total', 'ds': d, 'y': y, 'y_hat': y + np.random.normal(0, 15)} for d, y in zip(dates_h, y_total)\n",
    "] + [\n",
    "    {'unique_id': 'RegionA', 'ds': d, 'y': y, 'y_hat': y + np.random.normal(0, 5)} for d, y in zip(dates_h, y_a)\n",
    "] + [\n",
    "    {'unique_id': 'RegionB', 'ds': d, 'y': y, 'y_hat': y + np.random.normal(0, 5)} for d, y in zip(dates_h, y_b)\n",
    "])\n",
    "\n",
    "# 2. Define the Summing Matrix (S_df)\n",
    "S_df = pd.DataFrame(\n",
    "    [[1, 1],  # Total\n",
    "     [1, 0],  # RegionA\n",
    "     [0, 1]], # RegionB\n",
    "    columns=['RegionA', 'RegionB'],\n",
    "    index=['Total', 'RegionA', 'RegionB']\n",
    ")\n",
    "S_df.index.name = 'unique_id'\n",
    "S_df = S_df.reset_index()\n",
    "\n",
    "tags = {\n",
    "    'Total': ['Total'],\n",
    "    'Total/Regions': ['RegionA', 'RegionB']\n",
    "}\n",
    "\n",
    "# 3. Apply MinTrace Reconciliation\n",
    "Y_hat_df = df_h[['unique_id', 'ds', 'y_hat']]\n",
    "Y_df = df_h[['unique_id', 'ds', 'y']]\n",
    "\n",
    "reconcilers = [\n",
    "    BottomUp(),\n",
    "    MinTrace(method='ols') \n",
    "]\n",
    "\n",
    "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "Y_rec = hrec.reconcile(Y_hat_df=Y_hat_df, Y_df=Y_df, S_df=S_df, tags=tags)\n",
    "\n",
    "# 4. Check Coherency\n",
    "sample_date = dates_h[0]\n",
    "rec_values = Y_rec[Y_rec['ds'] == sample_date].set_index('unique_id')\n",
    "\n",
    "print(f\"\\nChecking Coherency for date: {sample_date.date()}\")\n",
    "mint_col = 'y_hat/MinTrace_method-ols'\n",
    "\n",
    "# Check if the column exists, otherwise fallback (just in case of version variance)\n",
    "if mint_col not in rec_values.columns:\n",
    "    # Try finding it dynamically if the name is slightly different\n",
    "    mint_col = [c for c in rec_values.columns if 'MinTrace' in c][0]\n",
    "\n",
    "print(f\"Using column: {mint_col}\")\n",
    "print(rec_values[['y_hat', mint_col]])\n",
    "\n",
    "total_mint = rec_values.loc['Total', mint_col]\n",
    "sum_bottom_mint = rec_values.loc['RegionA', mint_col] + rec_values.loc['RegionB', mint_col]\n",
    "\n",
    "print(f\"\\nMinTrace Total: {total_mint:.4f}\")\n",
    "print(f\"Sum of MinTrace Regions: {sum_bottom_mint:.4f}\")\n",
    "print(f\"Difference: {total_mint - sum_bottom_mint:.10f} (Should be near zero)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1571bf99-799a-4c78-a665-9fb74bb7ac8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
